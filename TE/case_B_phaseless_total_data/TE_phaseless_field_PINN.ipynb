{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2557c9-c762-4cc8-89a2-87c800a8bd96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3faac997-5af8-468e-ad71-87ac0d496cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wavenumber and wavelength\n",
    "k = 2.0 \n",
    "lamb =  2.0 * np.pi / k\n",
    "\n",
    "# domain [la,lb]\n",
    "la = -8.0 * lamb\n",
    "lb = 8.0 * lamb\n",
    "\n",
    "# define the mesh for observation\n",
    "N_obs = 240\n",
    "h_obs = (lb - la) / N_obs\n",
    "mesh_obs = np.linspace(la, lb, (N_obs + 1))\n",
    "\n",
    "mesh_mid_obs = np.zeros(N_obs + 1)\n",
    "\n",
    "mesh_mid_obs[0] = la\n",
    "\n",
    "for od in range(N_obs):\n",
    "    mesh_mid_obs[od + 1] = (mesh_obs[od] + mesh_obs[od+1])/2.0\n",
    "\n",
    "# transform numpy mesh to torch mesh\n",
    "x_mid_obs = torch.tensor(mesh_mid_obs, dtype = torch.float32, requires_grad = True)\n",
    "x_mesh_obs = torch.tensor(mesh_obs, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "# define the maximum number of testing points\n",
    "N_inv = 480\n",
    "\n",
    "# define the max and min of surface height\n",
    "hmax = 0.2 * lamb\n",
    "hmin = -0.2 * lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e8175c-323b-49e3-849a-262bc5c153ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# angle of grazing\n",
    "alpha = torch.tensor([[-2.0 * np.pi / 3.0]])\n",
    "\n",
    "# the incident wave, which is a plane wave\n",
    "def inc_field(x, z):\n",
    "    g = torch.cos(alpha) * x + torch.sin(alpha) * z\n",
    "    return torch.exp(1.0 * 1j * k * g)\n",
    "\n",
    "# observation height \n",
    "zp = lamb * 0.4\n",
    "\n",
    "# Hankel function of zeroth order of first kind\n",
    "def hankel(sth):\n",
    "        return torch.special.bessel_j0(sth) + 1j * torch.special.bessel_y0(sth)\n",
    "    \n",
    "# method of moments\n",
    "# note here this function produces the phaseless total field\n",
    "def MOM(x_mid, hei_vec, heid_vec):\n",
    "    ''' This function implements the method of moments.\n",
    "     x_mid is the mesh points.\n",
    "     hei_vec is the torch tensor for the surface height.\n",
    "     heid_vec is the torch tensor of the derivative of surface height.\n",
    "     it reaturns the torch tensor of phaseless total field.'''\n",
    "    \n",
    "    h = x_mid[100] - x_mid[99]\n",
    "    N = x_mid.numel() - 1\n",
    "    \n",
    "    # aseemble surface incident field\n",
    "    inc_vec =  torch.zeros(N, dtype=torch.cfloat)\n",
    "    for i in range(N):\n",
    "        inc_vec[i] = inc_field(x_mid[i+1], hei_vec[i+1])\n",
    "    \n",
    "    # aseemble matrix A\n",
    "    A =  torch.zeros((N, N), dtype=torch.cfloat)\n",
    "    \n",
    "    # off-diagonal terms\n",
    "    def getA_offdiag1(nn):\n",
    "        ll = torch.arange(1, nn)\n",
    "        ptl = 1.0 * k * torch.sqrt((x_mid[nn] - x_mid[ll]) ** 2 + (hei_vec[nn] - hei_vec[ll])**2)\n",
    "        sd = torch.sqrt(1.0 + heid_vec[ll]**2)\n",
    "        return 1.0 * h * 1j / 4.0 * hankel(ptl) * sd \n",
    "    def getA_offdiag2(nn):\n",
    "        ll = torch.arange(nn + 1, N + 1)\n",
    "        ptl = 1.0 * k * torch.sqrt((x_mid[nn] - x_mid[ll]) ** 2 + (hei_vec[nn] - hei_vec[ll])**2)\n",
    "        sd = torch.sqrt(1.0 + heid_vec[ll]**2)\n",
    "        return 1.0 * h * 1j / 4.0 * hankel(ptl) * sd \n",
    "    \n",
    "    # diagonal term\n",
    "    def getA_diag(nn):\n",
    "        gamma = 0.5772156649\n",
    "        term1 = 1.0 * 1j / 4.0 * h * (1.0 + 2.0 * 1j / torch.pi * gamma) \n",
    "        sd = torch.sqrt(1.0 + heid_vec[nn]**2)\n",
    "        term2 = - (1.0 * h) / (2.0 * torch.pi) * (torch.log(k * sd * h / 4.0) - 1.0)\n",
    "        return (term1 + term2) * sd\n",
    "    \n",
    "    # assemble matrix A\n",
    "    for nu in range(N):\n",
    "        A[nu, 0:nu] = getA_offdiag1(nu + 1)\n",
    "        A[nu, nu+1:N] = getA_offdiag2(nu + 1)\n",
    "        A[nu, nu] = getA_diag(nu + 1)\n",
    "    \n",
    "    # solve for surface current\n",
    "    phid = torch.linalg.solve(A, inc_vec)\n",
    "    \n",
    "    zz = torch.tensor([zp])\n",
    "    \n",
    "    # assemble matrix B\n",
    "    B =  torch.zeros((N, N), dtype=torch.cfloat)\n",
    "    \n",
    "    # components of matrix B\n",
    "    def getB(nn):\n",
    "        rr = torch.arange(1, N + 1) \n",
    "        ptl = 1.0 * k * torch.sqrt((x_mid[nn] - x_mid[rr]) ** 2 + (zz - hei_vec[rr])**2)\n",
    "        sd = torch.sqrt(1.0 + heid_vec[rr]**2)\n",
    "        return - 1.0 * h * 1j / 4.0 * hankel(ptl) * sd \n",
    "    \n",
    "    for nd in range(N):\n",
    "        B[nd, 0:N] = getB(nd + 1)\n",
    "\n",
    "    # calculate scattered field\n",
    "    phis = torch.matmul(B, phid)\n",
    "    \n",
    "    # assemble incident wave along measurement line\n",
    "    inc_vec_zz = torch.zeros(N, dtype=torch.cfloat)\n",
    "    for nt in range(N):\n",
    "        inc_vec_zz[nt] = inc_field(x_mid[nt+1], zz)\n",
    "    \n",
    "    # return the amplitude of total field\n",
    "    return torch.abs(phis + inc_vec_zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d97270e0-a91a-48c4-a294-a773cb0c861f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data of actual surface \n",
    "hei_actual_torch = torch.load('./vector_true_surface.pt')\n",
    "\n",
    "# load the data list of the interpolated scattered field\n",
    "# data without noise\n",
    "data_total_field_without_noise = torch.load('./data_phaseless_total_field_without_noise.pt')\n",
    "# data with noise\n",
    "data_total_field_with_noise = torch.load('data_phaseless_total_field_with_noise.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a43927ac-bcdb-49e9-a88f-384a503d460e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main structure for the neural network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # NL: the number of hidden layers\n",
    "    # NN: the number of vertices in each layer\n",
    "    def __init__(self, NL, NN):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(1, NN)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(NN, NN) for i in range(NL)])\n",
    "\n",
    "        self.output_layer = nn.Linear(NN, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = self.act(self.input_layer(x))\n",
    "\n",
    "        for i, li in enumerate(self.hidden_layers):\n",
    "            o = self.act(li(o))\n",
    "        \n",
    "        out = self.output_layer(o)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def act(self, x):\n",
    "        return x * torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918d1074-b634-40aa-9fb6-0b48be10f41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# at each epoch, we generate a mesh of size N_t, N_t is inside [N_obs, N_inv]\n",
    "def generate_mesh():\n",
    "    \n",
    "    # first generate a random number N_t\n",
    "    N_t = torch.randint(N_obs, N_inv + 1, (1,)).item()\n",
    "    \n",
    "    # identify the position in the data list\n",
    "    idx = N_t - N_obs\n",
    "    \n",
    "    # generate the mesh\n",
    "    x_mesh = torch.linspace(la,lb, N_t+1, requires_grad = True).view(-1,1)\n",
    "    x_train = torch.zeros(N_t + 1)\n",
    "    x_train[0] = la\n",
    "    for od in range(N_t):\n",
    "        x_train[od + 1] = (x_mesh[od] + x_mesh[od+1])/2.0\n",
    "        \n",
    "    return idx, x_train, N_t\n",
    "\n",
    "# normalization of a tensor\n",
    "def normalize_tensor(input_tensor):\n",
    "    \n",
    "    min_val = input_tensor.min()\n",
    "    max_val = input_tensor.max()\n",
    "    normalized_tensor = 2 * (input_tensor - min_val) / (max_val - min_val) - 1\n",
    "    \n",
    "    return normalized_tensor\n",
    "\n",
    "# the inverse normalization of a tensor\n",
    "def inverse_normalize_tensor(normalized_tensor, original_max, original_min):\n",
    "    \n",
    "    # Compute the scale and shift parameters\n",
    "    scale = (original_max - original_min) / 2\n",
    "    shift = (original_max + original_min) / 2\n",
    "    \n",
    "    inverse_normalized_tensor = normalized_tensor * scale + shift\n",
    "    return inverse_normalized_tensor\n",
    "\n",
    "# the automatic differentiation gives the derivative of normalization of output\n",
    "# w.r.t. normalizaton of input, applying the chain rule function leads to the\n",
    "# actual derivative of target output w.r.t. target input\n",
    "def chain_rule(derivative_tensor, output_max, output_min, input_max, input_min):\n",
    "    return (output_max - output_min) * derivative_tensor / (input_max - input_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e65130-fdcc-4c23-accc-0ff10ccfc516",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.06607130169868469\n",
      "Epoch 200, Loss: 0.06279180943965912\n",
      "Epoch 300, Loss: 0.04489124193787575\n",
      "Epoch 400, Loss: 0.03558586910367012\n",
      "Epoch 500, Loss: 0.036156922578811646\n",
      "Epoch 600, Loss: 0.3710682988166809\n",
      "Epoch 700, Loss: 0.039253316819667816\n",
      "Epoch 800, Loss: 0.03342565894126892\n",
      "Epoch 900, Loss: 0.01957095041871071\n",
      "Epoch 1000, Loss: 0.021058855578303337\n",
      "Epoch 1100, Loss: 0.008052891120314598\n",
      "Epoch 1200, Loss: 0.0010776676936075091\n",
      "Epoch 1300, Loss: 0.0004969916772097349\n",
      "Epoch 1400, Loss: 0.001431651646271348\n",
      "Epoch 1500, Loss: 0.00028632156318053603\n",
      "Epoch 1600, Loss: 0.0004582072142511606\n",
      "Epoch 1700, Loss: 0.0005113692604936659\n",
      "Epoch 1800, Loss: 0.0006212764419615269\n",
      "Epoch 1900, Loss: 0.0016915624728426337\n",
      "Epoch 2000, Loss: 0.0013159916270524263\n",
      "Epoch 2100, Loss: 0.0008687266381457448\n",
      "Epoch 2200, Loss: 0.0002966840111184865\n",
      "Epoch 2300, Loss: 0.00037571435677818954\n",
      "Epoch 2400, Loss: 0.000618560123257339\n",
      "Epoch 2500, Loss: 0.00037275574868544936\n",
      "Epoch 2600, Loss: 0.0009233730379492044\n",
      "Epoch 2700, Loss: 0.0011335528688505292\n",
      "Epoch 2800, Loss: 0.0010243068682029843\n",
      "Epoch 2900, Loss: 0.000950394372921437\n",
      "Epoch 3000, Loss: 0.00041984135168604553\n"
     ]
    }
   ],
   "source": [
    "# set up parameters for PINN neural networks\n",
    "# NL is number of hidden layers and NN is the number of neurons per hidden layer\n",
    "net = Net(NL = 6, NN = 512)\n",
    "# choose an optimizer and learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr = 1E-3)\n",
    "# set number of iterations\n",
    "N_PINN = 3000\n",
    "\n",
    "# this is the main loop for the PINN\n",
    "for t in range(N_PINN):\n",
    "    \n",
    "    # first step, generate N_t and the mesh\n",
    "    idx, x_train_ori, N_t = generate_mesh()\n",
    "    \n",
    "    # normalization of points\n",
    "    x_train = normalize_tensor(x_train_ori)\n",
    "    \n",
    "    # forward propagation\n",
    "    x_train = x_train.view(-1,1)\n",
    "    y = net(x_train)\n",
    "    \n",
    "    # use automatic differentiation to find derivatives\n",
    "    dy = torch.autograd.grad(y, x_train, grad_outputs= torch.ones(y.size()), create_graph = True)[0] \n",
    "    \n",
    "    # transform the surface to 1D vector and apply inverse normalization\n",
    "    hei_pred_net = y.view(-1)\n",
    "    hei_pred = inverse_normalize_tensor(hei_pred_net, hmax, hmin)\n",
    "    \n",
    "    # transform derivative to 1D vector and apply the chain rule\n",
    "    heid_pred_net = dy.view(-1)\n",
    "    heid_pred = chain_rule(heid_pred_net, hmax, hmin, x_train_ori.max(), x_train_ori.min())\n",
    "\n",
    "    # use MOM to calculate the total field\n",
    "    field_pred = MOM(x_train_ori, hei_pred, heid_pred)\n",
    "    \n",
    "    # identify the corresponding observed scattered data\n",
    "    field_obs = data_total_field_with_noise[idx]\n",
    "\n",
    "    # MSE loss of the residual of equation\n",
    "    loss_eqn = nn.MSELoss()(field_pred, field_obs)\n",
    "    \n",
    "    # find the predicted surface values on the close-boundary points\n",
    "    indices_bc = torch.tensor([0, 1, 2, 3, 4, N_t, N_t - 1, N_t - 2, N_t - 3, N_t - 4])\n",
    "    pred_bc = hei_pred[indices_bc]\n",
    "    \n",
    "    # MSE loss of boundary values\n",
    "    loss_bc = nn.MSELoss()(pred_bc, torch.zeros(10))\n",
    "    \n",
    "    # the total loss is the combination \n",
    "    loss = loss_eqn + loss_bc \n",
    "    \n",
    "    # optimizing step with backward propagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # print(f\"Epoch {t}, Loss: {loss.item()}\")\n",
    "    if (t + 1) % 100 == 0:\n",
    "        print(f\"Epoch {t + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad56230-d7c1-4117-9a10-b05ca93ee6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalization of testing points\n",
    "x_mesh_test = normalize_tensor(x_mesh_obs)\n",
    "\n",
    "# forward propagation and inverse normalization to obtain the result surface height\n",
    "y = net(x_mesh_test.view(-1,1)).view(-1)\n",
    "hei_pred_output = inverse_normalize_tensor(y, hmax, hmin)\n",
    "\n",
    "# transform torch tensors to numpy arrays\n",
    "hei_actual_vec = hei_actual_torch.detach().numpy()\n",
    "hei_pred_vec = hei_pred_output.detach().numpy()\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=[8, 6])\n",
    "line1, = plt.plot(mesh_mid_obs / lamb, hei_actual_vec / lamb, color='r', linewidth = 2)\n",
    "line2, = plt.plot(mesh_mid_obs / lamb, hei_pred_vec / lamb, color='b', linewidth=2, linestyle = '--', fillstyle = 'none')\n",
    "plt.xlabel(r'$x/{\\lambda}$', fontsize = 15)\n",
    "plt.ylabel(r'$z/{\\lambda}$', fontsize = 15)\n",
    "leg=plt.legend([line1, line2],\n",
    "        ['actual surface',\n",
    "        'reconstructed surface'],\n",
    "        prop={'size': 15}, loc = \"best\")\n",
    "plt.gca().tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771ca42-2861-49eb-9ee9-d9ce1c2938e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the l2 norm error\n",
    "error_vec = hei_actual_vec - hei_pred_vec\n",
    "print(\"l2 norm error is: \", np.linalg.norm(error_vec) / np.linalg.norm(hei_actual_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a4c9ad-e1e5-40fe-b708-fee8dafba20e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
